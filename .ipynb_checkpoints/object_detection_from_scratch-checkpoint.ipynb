{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for data and feature extraction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [function] yxyx convert to hwyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yxyx_hwyx(yxyx):\n",
    "    height = yxyx[:, 2] - yxyx[:, 0]\n",
    "    width  = yxyx[:, 3] - yxyx[:, 1]\n",
    "    ctr_y  = yxyx[:, 0] + 0.5 * width\n",
    "    ctr_x  = yxyx[:, 1] + 0.5 * width\n",
    "    return height,width,ctr_y,ctr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [function] calculate iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(proposed_bboxes, gt_bboxes):\n",
    "    for num1, i in enumerate(proposed_bboxes):\n",
    "        ya1, xa1, ya2, xa2 = i\n",
    "        anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "\n",
    "        for num2, j in enumerate(gt_bboxes):\n",
    "            yb1, xb1, yb2, xb2 = j\n",
    "            box_area = (yb2 - yb1) * (xb2 - xb1)\n",
    "            inter_x1 = max([xb1, xa1])\n",
    "            inter_y1 = max([yb1, ya1])\n",
    "            inter_x2 = min([xb2, xa2])\n",
    "            inter_y2 = min([yb2, ya2])\n",
    "\n",
    "            if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "                iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "                iou = iter_area / (anchor_area + box_area - iter_area)\n",
    "            else:\n",
    "                iou = 0.\n",
    "\n",
    "            ious[num1, num2] = iou\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gt_bboxes, gt_labels, ssr, dummy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 800])\n"
     ]
    }
   ],
   "source": [
    "gt_bboxes = np.asarray([[20, 30, 400, 500], [300, 400, 500, 600]], dtype=np.float32) # [y1, x1, y2, x2] formatgt_\n",
    "gt_labels = np.asarray([6, 8], dtype=np.int8) # 0 represents background\n",
    "sub_sample_ratio = 16\n",
    "\n",
    "image = torch.zeros((1, 3, 800, 800)).float()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  VGG16 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (17): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (24): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "fe    = list(model.features)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get output layer size (ssr 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "req_features = []\n",
    "k = Variable(image.clone())\n",
    "\n",
    "for i in range(len(fe)):\n",
    "    k = model.features[i](k)\n",
    "    if k.size()[2] < 800//16:\n",
    "        break\n",
    "    req_features.append(model.features[i])\n",
    "    out_channels = k.size()[1]\n",
    "    \n",
    "print(out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert this list into a Sequential module and extract the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = nn.Sequential(*req_features)\n",
    "out_map = feature_extractor(Variable(image))\n",
    "\n",
    "print(out_map.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Network\n",
    "\n",
    "This step is to prepare anchor boxes for RPN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate anchor base unit\n",
    "\n",
    "- anchor_base is the base unit for anchor. It has 9 different anchors for 1 feature unit (if subsample is 16, then it correspondings to 16 pixels sub-region)\n",
    "- anchor_base unit is the same as original image unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -37.254833  -82.50967    53.254833   98.50967 ]\n",
      " [ -82.50967  -173.01933    98.50967   189.01933 ]\n",
      " [-173.01933  -354.03867   189.01933   370.03867 ]\n",
      " [ -56.        -56.         72.         72.      ]\n",
      " [-120.       -120.        136.        136.      ]\n",
      " [-248.       -248.        264.        264.      ]\n",
      " [ -82.50967   -37.254833   98.50967    53.254833]\n",
      " [-173.01933   -82.50967   189.01933    98.50967 ]\n",
      " [-354.03867  -173.01933   370.03867   189.01933 ]]\n"
     ]
    }
   ],
   "source": [
    "ratios        = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "\n",
    "anchor_base   = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\n",
    "\n",
    "ctr_y = sub_sample_ratio / 2.\n",
    "ctr_x = sub_sample_ratio / 2.\n",
    "\n",
    "for i in range(len(ratios)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample_ratio * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "        w = sub_sample_ratio * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "\n",
    "        index = i * len(anchor_scales) + j\n",
    "\n",
    "        anchor_base[index, 0] = ctr_y - h / 2.\n",
    "        anchor_base[index, 1] = ctr_x - w / 2.\n",
    "        anchor_base[index, 2] = ctr_y + h / 2.\n",
    "        anchor_base[index, 3] = ctr_x + w / 2.\n",
    "        \n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate anchors\n",
    "- ctr_x and ctr_y is the central point coordinate for each sub-region of the original image. 2 is the ctr_x and ctr_y\n",
    "- ctr shape = (2500,2). 2500 is the number of central point for each sub-region of the original image.\n",
    "- number of anchors is equal to the number central point multiply with number of possible anchor proposal. so its 50\\*50\\*9 = 22500\n",
    "- anchors format is y1,x1,y2,x2\n",
    "- the only difference from the above base unit calculation is this part. Its the shift of central points\n",
    "```python\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2)\n",
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "fe_output_size = (800//16)\n",
    "\n",
    "ctr_x = np.arange(16, (fe_output_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_output_size+1) * 16, 16)\n",
    "ctr   = np.zeros([fe_output_size**2, 2])\n",
    "\n",
    "index = 0\n",
    "\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[x] - 8\n",
    "        ctr[index, 0] = ctr_y[y] - 8\n",
    "        index +=1\n",
    "        \n",
    "print(ctr.shape)\n",
    "\n",
    "anchors = np.zeros([(fe_output_size * fe_output_size * 9), 4])\n",
    "index = 0\n",
    "\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            h = sub_sample_ratio * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample_ratio * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "            anchors[index, 0] = ctr_y - h / 2.\n",
    "            anchors[index, 1] = ctr_x - w / 2.\n",
    "            anchors[index, 2] = ctr_y + h / 2.\n",
    "            anchors[index, 3] = ctr_x + w / 2.\n",
    "            index += 1\n",
    "            \n",
    "print(anchors.shape)\n",
    "#Out: [22500, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign labels and location of objects (with respect to the anchor) to each and every anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidelines to assign labels to the anchor boxes:\n",
    "- The anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth-box or \n",
    "- An anchor that has an IoU overlap higher than 0.7 with ground-truth box.\n",
    "- We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. \n",
    "- Anchors that are neither positive nor negitive do not contribute to the training objective. (0.3-0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assign the labels and locations for the anchor boxes in the following ways.\n",
    "- Find the indexes of valid anchor boxes and create an array with these indexes. Create an label array with shape index array filled with -1.\n",
    "- Check weather one of the above conditions a, b, c is statisfying or not and fill the label accordingly. In the case for positive anchor box (label is 1), we will mark the associated ground truth object.\n",
    "- Calculate the locations (loc) of ground truth associated with the anchor box wrt to the anchor box.\n",
    "- Reorganize all anchor boxes by filling with -1 for all unvalid anchor boxes and values we have calculated for all valid anchor boxes.\n",
    "- Outputs should be labels with (N, 1) array and locs with (N, 4) array.\n",
    "- Find the index of all valid anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [filter_1] filter out-of-image anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "(8940, 4)\n"
     ]
    }
   ],
   "source": [
    "# filter out those anchors whose size fall outside the range of the orginal image size.\n",
    "inside_index = np.where((anchors[:, 0] >= 0) &\n",
    "                        (anchors[:, 1] >= 0) &\n",
    "                        (anchors[:, 2] <= 800) &\n",
    "                        (anchors[:, 3] <= 800)\n",
    "                       )[0]\n",
    "print(inside_index.shape)\n",
    "\n",
    "inside_anchors = anchors[inside_index]\n",
    "print(inside_anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have filter out 22500-8940 = 13560 anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate iou of inside_anchors and gt_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(inside_anchors), len(gt_bboxes)), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "ious = get_iou(inside_anchors,gt_bboxes)\n",
    "        \n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. The sudo code for calculating iou between two boxes will be\n",
    "\n",
    "- Find the max of x1 and y1 in both the boxes (xn1, yn1)\n",
    "- Find the min of x2 and y2 in both the boxes (xn2, yn2)\n",
    "- Now both the boxes are intersecting only\n",
    "\n",
    "```python\n",
    "if (xn1 < xn2) and (yn2 < yn1):\n",
    "    iou_area will be (xn2 - xn1) * (yn2 - yn1)\n",
    "else:\n",
    "    iuo_area will be 0\n",
    "```\n",
    "      \n",
    "- similarly calculate area for anchor box and ground truth object\n",
    "- iou = iou_area/(anchor_box_area + ground_truth_area - iou_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [filter_2] filter by iou threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the scenarios of a and b, we need to find two things here\n",
    "- the highest iou for each gt_box and its corresponding anchor box\n",
    "- the highest iou for each anchor box and its corresponding ground truth box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_argmax_ious: (18,)\n",
      "argmax_ious: (8940,)\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious_ = ious.argmax(axis=0)\n",
    "gt_max_ious     = ious[gt_argmax_ious_, np.arange(ious.shape[1])]\n",
    "gt_argmax_ious  = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "argmax_ious = ious.argmax(axis=1)\n",
    "max_ious    = ious[np.arange(len(ious)), argmax_ious]\n",
    "\n",
    "print(f'gt_argmax_ious: {gt_argmax_ious.shape}')\n",
    "print(f'argmax_ious: {argmax_ious.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gt_argmax_ious = maximum overlapped anchor box w.r.t. ground truth box \n",
    "- argmax_ious = maximum overlapped ground truth box w.r.t. anchor box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three arrays\n",
    "- argmax_ious — Tells which ground truth object has max iou with each anchor.\n",
    "- max_ious — Tells the max_iou with ground truth object with each anchor.\n",
    "- gt_argmax_ious — Tells the anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_iou_threshold is the threshold for selecting positive anchor box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "# so max_ious is only used to assign label for the anchor box\n",
    "anchor_labels_ = np.empty((len(inside_index), ), dtype=np.int32)\n",
    "anchor_labels_.fill(-1)\n",
    "print(anchor_labels_.shape)\n",
    "\n",
    "anchor_labels_[max_ious < neg_iou_threshold]  = 0 # assign negative label with background label 0\n",
    "anchor_labels_[gt_argmax_ious]                = 1\n",
    "anchor_labels_[max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RPN The Faster_R-CNN paper phrases as follows Each mini-batch arises from a single image that contains many positive and negitive example anchors, but this will bias towards negitive samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negitive ones.. From this we can derive two variable as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [filter_3] filter with pos/neg ratio\n",
    "- making excessive pos/neg labels to be ignored label so that we can maintain a proper pos/neg ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratio = 0.5\n",
    "n_sample  = 256\n",
    "\n",
    "n_pos     = pos_neg_ratio * n_sample # 128\n",
    "n_neg     = n_sample - np.sum(anchor_labels_ == 1) #\n",
    "\n",
    "pos_index = np.where(anchor_labels_ == 1)[0] # 18\n",
    "neg_index = np.where(anchor_labels_ == 0)[0] # 7690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filter_3 - positive: 18\n",
      "before filter_3 - ignored: 1232\n",
      "before filter_3 - negative: 7690\n"
     ]
    }
   ],
   "source": [
    "print(f\"before filter_3 - positive: {np.sum(anchor_labels_ == 1)}\")\n",
    "print(f\"before filter_3 - ignored: {np.sum(anchor_labels_ == -1)}\")\n",
    "print(f\"before filter_3 - negative: {np.sum(anchor_labels_ == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    anchor_labels_[disable_index] = -1\n",
    "\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "    anchor_labels_[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filter_3 - positive: 18\n",
      "after filter_3 - ignored: 8684\n",
      "after filter_3 - negative: 238\n"
     ]
    }
   ],
   "source": [
    "print(f\"after filter_3 - positive: {np.sum(anchor_labels_ == 1)}\")\n",
    "print(f\"after filter_3 - ignored: {np.sum(anchor_labels_ == -1)}\")\n",
    "print(f\"after filter_3 - negative: {np.sum(anchor_labels_ == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive sample: 18\n",
      "Number of negative sample: 238\n",
      "Number of positive/negative sample: 7.56%\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of positive sample: {np.sum(anchor_labels_ == 1)}')\n",
    "print(f'Number of negative sample: {np.sum(anchor_labels_ == 0)}')\n",
    "print(f'Number of positive/negative sample: {round(np.sum(anchor_labels_ == 1)/np.sum(anchor_labels_ == 0)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find amount of shift to move anchors closer to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above formulas to find the loc\n",
    "- We have found the gt_bboxes which are closer to the anchors, Now we need to know how to shift them closer\n",
    "- Basically dy,dx,dh,dw means that how much shift we need to move our anchor proposals closer to the most overlapped ground truth bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/demo_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 4)\n",
      "[[ 1.08416503  2.30914558  0.7415674   1.64727602]\n",
      " [ 0.99577668  2.30914558  0.7415674   1.64727602]\n",
      " [ 0.90738834  2.30914558  0.7415674   1.64727602]\n",
      " ...\n",
      " [-2.00942714 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.09781548 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.18620383 -5.29225232  0.7415674   1.64727602]]\n"
     ]
    }
   ],
   "source": [
    "# find gt_bboxes that are closer to the anchors\n",
    "max_iou_gt_bbox = gt_bboxes[argmax_ious]\n",
    "print(max_iou_gt_bbox.shape)\n",
    "\n",
    "base_height,base_width,base_ctr_y,base_ctr_x = yxyx_hwyx(max_iou_gt_bbox)\n",
    "height     ,width     ,ctr_y     ,ctr_x      = yxyx_hwyx(inside_anchors)\n",
    "\n",
    "eps    = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width  = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "anchor_shift_ = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "\n",
    "print(anchor_shift_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### convert filtered anchors_labels and anchor_locs to full size anchor tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_labels = np.empty((len(anchors),), dtype=anchor_labels_.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[inside_index] = anchor_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_shift = np.empty(anchors.shape, dtype=anchor_shift_.dtype)\n",
    "anchor_shift.fill(0)\n",
    "anchor_shift[inside_index, :] = anchor_shift_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final two matrices are\n",
    "- anchor_locations [N, 4] — [22500, 4]. This shows how close the anchors are to the most matched grounth truth box\n",
    "- anchor_labels [N,] — [22500,]. This shows the labels that respect with the pos/neg ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pred_anchor_shift, pred_cls_scores, pred_objectness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_channels = 512\n",
    "in_channels  = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "\n",
    "n_anchor  = len(ratios) * len(anchor_scales) # Number of anchors at each location\n",
    "\n",
    "conv1     = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor *4, kernel_size=1, stride=1, padding=0)\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor *2, kernel_size=1, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 18]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv sliding layer\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "# Regression layer\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "# classification layer\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_map\t\t: torch.Size([1, 512, 50, 50])\n",
      "x\t\t: torch.Size([1, 512, 50, 50])\n",
      "pred_cls_scores\t: torch.Size([1, 18, 50, 50])\n",
      "pred_anchor_locs: torch.Size([1, 36, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "x = conv1(out_map) # out_map is obtained in section 1, its the output of the feature extraction \n",
    "\n",
    "pred_anchor_shift  = reg_layer(x)\n",
    "pred_anchor_scores = cls_layer(x)\n",
    "\n",
    "# pred_anchor_scores  = 9 * 2\n",
    "# pred_anchor_shift = 9 * 4\n",
    "print(f'out_map\\t\\t: {out_map.shape}')\n",
    "print(f'x\\t\\t: {x.shape}')\n",
    "print(f'pred_cls_scores\\t: {pred_anchor_scores.shape}')\n",
    "print(f'pred_anchor_locs: {pred_anchor_shift.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_anchor_shift: torch.Size([1, 22500, 4])\n",
      "pred_objectness_score: torch.Size([1, 22500])\n",
      "pred_cls_scores: torch.Size([1, 22500, 2])\n"
     ]
    }
   ],
   "source": [
    "pred_anchor_shift = pred_anchor_shift.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(f'pred_anchor_shift: {pred_anchor_shift.shape}')\n",
    "\n",
    "pred_anchor_scores = pred_anchor_scores.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "pred_objectness_score = pred_anchor_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(f'pred_objectness_score: {pred_objectness_score.shape}')\n",
    "\n",
    "pred_anchor_scores = pred_anchor_scores.view(1, -1, 2)\n",
    "print(f'pred_cls_scores: {pred_cls_scores.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_thresh = 0.7\n",
    "\n",
    "n_train_pre_nms  = 12000\n",
    "n_train_post_nms = 2000\n",
    "\n",
    "n_test_pre_nms  = 6000\n",
    "n_test_post_nms = 300\n",
    "\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_height, anc_width, anc_ctr_y, anc_ctr_x = yxyx_hwyx(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predictions locs using above formulas. before that convert the pred_anchor_locs and objectness_score to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_anchor_shift_numpy     = pred_anchor_shift[0].data.numpy()\n",
    "pred_objectness_score_numpy = pred_objectness_score[0].data.numpy()\n",
    "\n",
    "dy = pred_anchor_shift_numpy[:, 0::4]\n",
    "dx = pred_anchor_shift_numpy[:, 1::4]\n",
    "dh = pred_anchor_shift_numpy[:, 2::4]\n",
    "dw = pred_anchor_shift_numpy[:, 3::4]\n",
    "\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert [ctr_x, ctr_y, h, w] to [y1, x1, y2, x2] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = np.zeros(pred_anchor_shift_numpy.shape, dtype=anchor_shift.dtype)\n",
    "\n",
    "roi[:, 0::4] = ctr_y - 0.5 * h\n",
    "roi[:, 1::4] = ctr_x - 0.5 * w\n",
    "roi[:, 2::4] = ctr_y + 0.5 * h\n",
    "roi[:, 3::4] = ctr_x + 0.5 * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [filter_1] clip boundary of rois to the boundary of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.13426725   0.          97.73555076  95.09809707]\n",
      " [  4.95200533   0.         182.67525006 180.85454262]\n",
      " [  8.3521118    0.         372.62880435 387.87483792]\n",
      " ...\n",
      " [654.94896192 745.03028222 800.         800.        ]\n",
      " [518.51445941 706.26263585 800.         800.        ]\n",
      " [247.30084232 616.24451306 800.         800.        ]]\n",
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "img_size = image.shape[-2:]  # Image size\n",
    "\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "print(roi)\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [filter_2] remove rois that are smaller than the anchor box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22352,)\n",
      "(22352, 4)\n"
     ]
    }
   ],
   "source": [
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "\n",
    "# filter rois that are largest than the min_size (anchor base size)\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "\n",
    "roi    = roi[keep, :]\n",
    "scores = pred_objectness_score_numpy[keep]\n",
    "\n",
    "print(scores.shape)\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [filter_3] take top pre_nms_topN (e.g. 12000 while training and 300 while testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 4)\n",
      "[[208.27000728   0.         800.         221.68423927]\n",
      " [192.67510795   0.         800.         222.66425971]\n",
      " [176.3775566    0.         800.         222.93911448]\n",
      " ...\n",
      " [  0.         635.97600923 240.77252393 800.        ]\n",
      " [728.43206791 542.95181103 800.         800.        ]\n",
      " [425.9314086  635.98000762 784.76956353 800.        ]]\n"
     ]
    }
   ],
   "source": [
    "ordered_scores = scores.ravel().argsort()[::-1]\n",
    "ordered_scores = ordered_scores[:n_train_pre_nms]\n",
    "roi = roi[ordered_scores, :]\n",
    "print(roi.shape)\n",
    "print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [filter_4] non-maximum suppression. take 2000 most overlapped roi, after sort them by objectness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n"
     ]
    }
   ],
   "source": [
    "y1 = roi[:, 0]\n",
    "x1 = roi[:, 1]\n",
    "y2 = roi[:, 2]\n",
    "x2 = roi[:, 3]\n",
    "\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "order = ordered_scores.argsort()[::-1]\n",
    "keep = []\n",
    "\n",
    "while order.size > 0:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "    \n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "    \n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "    \n",
    "    inter = w * h\n",
    "    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "    \n",
    "keep = keep[:n_train_post_nms]  # while training/testing , use accordingly\n",
    "roi = roi[keep]  # the final region proposals for training\n",
    "\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 128\n",
    "pos_ratio = 0.25\n",
    "\n",
    "pos_iou_thresh    = 0.5\n",
    "neg_iou_thresh_hi = 0.5\n",
    "neg_iou_thresh_lo = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate iou of roi and ground truth boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(roi), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "ious = get_iou(roi,gt_bboxes)        \n",
    "        \n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [filter_1] filter by iou threshold and pos/neg ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 ... 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_ious      = ious.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive threshold. Positive index selection is bounded by minimum of pos_ratio or pos_iou_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[1882 1439 1554 1667 1762 1544  931 1043 1992 1779  830  983 1093 1648\n",
      " 1870  825  991  921]\n"
     ]
    }
   ],
   "source": [
    "pos_roi_per_image      = int(n_samples * pos_ratio)\n",
    "pos_index              = np.where(max_ious >= pos_iou_thresh)[0]\n",
    "\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "    \n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative threshold. Negative index selection is bounded by minimum of (sample-pos_roi_per_this_image) or neg_iou_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "[ 163  622  557 1662  715  616 1973  297 1460  660 1226  151 1112  836\n",
      " 1123   35   80 1241  215  223   73  536 1350   94  768  366  903 1208\n",
      "  802  868   41 1246  632 1772  240 1546  646  935  741  585  520  887\n",
      " 1243  140 1488 1687 1853  348 1676 1115  688  451   15 1086 1521 1887\n",
      "  990 1798  110  388  859  708 1944  436 1845  936 1858 1739  415 1705\n",
      " 1257  683  701  188 1454   39 1755 1267  792 1421 1428  740 1450  780\n",
      "  969  157 1341 1490 1526  810  563  949 1920 1629 1181 1315  496 1370\n",
      "  831 1596   53  238  257  178  674 1799  611 1721 1893 1806]\n"
     ]
    }
   ],
   "source": [
    "neg_index              = np.where((max_ious < neg_iou_thresh_hi) & \n",
    "                                  (max_ious >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_samples - pos_roi_per_this_image\n",
    "\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "\n",
    "if neg_index.size > 0:\n",
    "    neg_index = np.random.choice(neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "    \n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we gather positve samples index and negitive samples index, their respective labels and region proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed: (128, 4)\n",
      "Ground Truth: (128, 4)\n"
     ]
    }
   ],
   "source": [
    "keep_index_pos_neg = np.append(pos_index, neg_index)\n",
    "\n",
    "gt_roi_label_max = gt_labels[gt_assignment]\n",
    "gt_roi_labels    = gt_roi_label_max[keep_index_pos_neg]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "\n",
    "# roi after iou_threshold and positive ratio filter\n",
    "roi_pos_neg      = roi[keep_index_pos_neg]\n",
    "print(f'Proposed: {roi_pos_neg.shape}')\n",
    "\n",
    "# gt_bbox after iou_threshold and positive ratio filter\n",
    "gt_roi_pos_neg   = gt_bboxes[gt_assignment[keep_index_pos_neg]]\n",
    "print(f'Ground Truth: {gt_roi_pos_neg.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find amount of shift to move rois closer to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "height,      width,      ctr_y,      ctr_x      = yxyx_hwyx(roi_pos_neg)\n",
    "base_height, base_width, base_ctr_y, base_ctr_x = yxyx_hwyx(gt_roi_pos_neg)\n",
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width  = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "gt_roi_shift = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "\n",
    "print(gt_roi_shift.shape)\n",
    "\n",
    "#Out:\n",
    "# [[-0.08075945, -0.14638858, -0.23822695, -0.23150307],\n",
    "#  [ 0.04865225,  0.15570255,  0.08902431, -0.5969549 ],\n",
    "#  [ 0.17411101,  0.2244332 ,  0.19870323,  0.25063717],\n",
    "#  .....\n",
    "#  [-0.13976236,  0.121031  ,  0.03863466,  0.09662855],\n",
    "#  [-0.59361845, -2.5121436 ,  0.04558792,  0.9731178 ],\n",
    "#  [ 0.1041566 , -0.7840459 ,  1.4283055 ,  0.95092565]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create indices_and_rois combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5])\n"
     ]
    }
   ],
   "source": [
    "rois = torch.from_numpy(roi_pos_neg).float()\n",
    "\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "\n",
    "yxyx_indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xyxy_indices_and_rois = yxyx_indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois      = xyxy_indices_and_rois.contiguous()\n",
    "indices_and_rois[:, 1:].mul_(1 / 16.0)  # Subsampling ratio skipping the index\n",
    "indices_and_rois      = indices_and_rois.long()\n",
    "\n",
    "print(indices_and_rois.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate roi feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 512, 7, 7])\n",
      "torch.Size([128, 25088])\n"
     ]
    }
   ],
   "source": [
    "size = 7  # max pool 7x7\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "roi_feature_ = []\n",
    "\n",
    "num_rois = indices_and_rois.size(0)\n",
    "\n",
    "for i in range(num_rois):\n",
    "    roi = indices_and_rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4] + 1), roi[1]:(roi[3] + 1)]\n",
    "    roi_feature_.append(adaptive_max_pool(im))\n",
    "    \n",
    "roi_feature_ = torch.cat(roi_feature_, 0)\n",
    "print(roi_feature_.shape)\n",
    "\n",
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "roi_feature = output.view(output.size(0), -1)\n",
    "print(roi_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification for shift and class score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roi_cls_shift\t\t: torch.Size([128, 84])\n",
      "roi_cls_score\t\t: torch.Size([128, 21])\n",
      "pred_anchor_shift\t: torch.Size([1, 22500, 4])\n",
      "pred_anchor_scores\t: torch.Size([1, 22500, 2])\n",
      "anchor_shift\t\t: (22500, 4)\n",
      "anchor_labels\t\t: (22500,)\n"
     ]
    }
   ],
   "source": [
    "roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096),\n",
    "                                      nn.Linear(4096, 4096)])\n",
    "cls_shift = nn.Linear(4096, 21 * 4)  # (VOC 20 classes + 1 background. Each will have 4 co-ordinates)cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_shift.bias.data.zero_()\n",
    "cls_score = nn.Linear(4096, 21)  # (VOC 20 classes + 1 background)\n",
    "\n",
    "roi_feature_  = roi_head_classifier(roi_feature)\n",
    "roi_shift = cls_shift(roi_feature_)\n",
    "roi_score = cls_score(roi_feature_)\n",
    "\n",
    "print(f'roi_cls_shift\\t\\t: {roi_shift.shape}')\n",
    "print(f'roi_cls_score\\t\\t: {roi_score.shape}')\n",
    "\n",
    "print(f'pred_anchor_shift\\t: {pred_anchor_shift.shape}')\n",
    "print(f'pred_anchor_scores\\t: {pred_anchor_scores.shape}')\n",
    "\n",
    "print(f'anchor_shift\\t\\t: {anchor_shift.shape}')\n",
    "print(f'anchor_labels\\t\\t: {anchor_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RPN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpn_shift\t: torch.Size([22500, 4])\n",
      "rpn_score\t: torch.Size([22500, 2])\n",
      "gt_rpn_shift\t: torch.Size([22500, 4])\n",
      "gt_rpn_score\t: torch.Size([22500])\n"
     ]
    }
   ],
   "source": [
    "rpn_shift  = pred_anchor_shift[0]\n",
    "rpn_score  = pred_anchor_scores[0]\n",
    "\n",
    "gt_rpn_shift = torch.from_numpy(anchor_shift)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "\n",
    "print(f'rpn_shift\\t: {rpn_shift.shape}')\n",
    "print(f'rpn_score\\t: {rpn_score.shape}')\n",
    "\n",
    "print(f'gt_rpn_shift\\t: {gt_rpn_shift.shape}')\n",
    "print(f'gt_rpn_score\\t: {gt_rpn_score.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rpn label loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6919\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rpn_label_loss = F.cross_entropy(rpn_score, Variable(gt_rpn_score.long()), ignore_index=-1)\n",
    "print(rpn_label_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rpn shift loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22500, 4])\n"
     ]
    }
   ],
   "source": [
    "pos  = gt_rpn_score > 0\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_shift)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_shift_preds\t: torch.Size([18, 4])\n",
      "mask_shift_targets\t: torch.Size([18, 4])\n"
     ]
    }
   ],
   "source": [
    "mask_shift_preds   = rpn_shift[mask].view(-1, 4).float()\n",
    "mask_shift_targets = Variable(gt_rpn_shift[mask].view(-1, 4).float())\n",
    "\n",
    "print(f'mask_shift_preds\\t: {mask_shift_preds.shape}') \n",
    "print(f'mask_shift_targets\\t: {mask_shift_targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.1620\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_rpn = torch.abs(mask_shift_targets - mask_shift_preds)\n",
    "rpn_shift_loss = ((x_rpn < 1).float() * 0.5 * x_rpn**2) + ((x_rpn >= 1).float() * (x_rpn-0.5))\n",
    "print(rpn_shift_loss.sum())\n",
    "\n",
    "N_positive_label_rpn = (gt_rpn_score > 0).float().sum()\n",
    "rpn_shift_loss = rpn_shift_loss.sum() / N_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### rpn total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 7.6111\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rpn_lambda = 10.\n",
    "rpn_total_loss = rpn_label_loss + (rpn_lambda * rpn_label_loss)\n",
    "\n",
    "print(rpn_total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROI Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_roi_shift\t: torch.Size([128, 4])\n",
      "gt_roi_label\t: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "gt_roi_shift = torch.from_numpy(gt_roi_shift)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "\n",
    "print(f'gt_roi_shift\\t: {gt_roi_shift.shape}')\n",
    "print(f'gt_roi_label\\t: {gt_roi_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### roi label loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3.0382\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roi_label_loss = F.cross_entropy(roi_score, Variable(gt_roi_label), ignore_index=-1)\n",
    "print(roi_label_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### roi shift loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 21, 4])\n"
     ]
    }
   ],
   "source": [
    "n_sample   = roi_shift.shape[0]\n",
    "roi_shift_ = roi_shift.view(n_sample, -1, 4)\n",
    "print(roi_shift_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 21, 4])\n",
      "torch.Size([128, 4])\n",
      "Variable containing:\n",
      " 394.6296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_sample   = roi_shift.shape[0]\n",
    "roi_shift_ = roi_shift.view(n_sample, -1, 4)\n",
    "print(roi_shift_.shape)\n",
    "\n",
    "roi_shift_sample = roi_shift_[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_shift_sample.shape)\n",
    "\n",
    "x_roi = torch.abs(Variable(gt_roi_shift.float()) - roi_shift_sample)\n",
    "roi_shift_loss = ((x_roi < 1).float() * 0.5 * x_roi ** 2) + ((x_roi >= 1).float() * (x_roi - 0.5))\n",
    "print(roi_shift_loss.sum())\n",
    "\n",
    "N_positive_label_roi = (gt_rpn_score > 0).float().sum()\n",
    "roi_shift_loss = roi_shift_loss.sum() / N_positive_label_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### roi total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 15.2181\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roi_lambda = 10.\n",
    "roi_total_loss = roi_label_loss + (roi_lambda * roi_shift_loss)\n",
    "\n",
    "print(roi_total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RPN Loss + ROI Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 22.8292\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_loss = rpn_total_loss + roi_total_loss\n",
    "print(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "720.724px",
    "left": "1375.45px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
